# .env.example
# Mira Backend Environment Configuration - Gemini 3 Pro + Gemini Embeddings
# Copy this file to .env and fill in your actual values

# ============================================================================
# GEMINI 3 PRO (PRIMARY LLM)
# ============================================================================
# Gemini 3 Pro single-model with variable thinking levels
# - Thinking levels: low, high
# - Cost varies by thinking level and token usage
#
# API Limits (Tier 1):
#   Context Window: 1,000,000 tokens
#   Max Output: 65,536 tokens
#   RPM: 50, TPM: 1,000,000, RPD: 1,000
#
# Pricing (per 1M tokens):
#   <200k context: $2 input / $12 output
#   >200k context: $4 input / $18 output

# Google API key (used for both Gemini 3 Pro and Gemini embeddings)
GOOGLE_API_KEY=your-google-api-key-here

# Model name
GEMINI_MODEL=gemini-3-pro-preview

# Default thinking level: low or high
# Can be overridden per-request based on complexity
GEMINI_THINKING_LEVEL=high

# ============================================================================
# GEMINI EMBEDDINGS
# ============================================================================
# Used for gemini-embedding-001 embeddings
GEMINI_EMBEDDING_MODEL=gemini-embedding-001

# ============================================================================
# BUDGET MANAGEMENT
# ============================================================================
# Daily and monthly spending limits (USD)
BUDGET_DAILY_LIMIT_USD=5.0
BUDGET_MONTHLY_LIMIT_USD=150.0

# ============================================================================
# CONTEXT BUDGET
# ============================================================================
# Gemini 3 Pro has tiered pricing based on context size:
#   <200k context: $2 input / $12 output per 1M tokens
#   >200k context: $4 input / $18 output per 1M tokens (2x input, 1.5x output)
#
# Enforce staying in standard pricing tier (truncate context to <200k)
ENFORCE_STANDARD_PRICING_TIER=false
# Maximum context tokens (0 = no limit, uses full 1M context window)
MAX_CONTEXT_TOKENS=0
# Enable proactive warnings when approaching 200k threshold
ENABLE_CONTEXT_WARNINGS=true
# Warning threshold as percent of 200k (90 = warn at 180k)
CONTEXT_WARNING_THRESHOLD_PERCENT=90

# ============================================================================
# LLM RESPONSE CACHE
# ============================================================================
# Enable caching to reduce API costs (target 80%+ hit rate)
CACHE_ENABLED=true

# Time-to-live for cached responses (seconds, default 24 hours)
CACHE_TTL_SECONDS=86400

# ============================================================================
# STRUCTURED OUTPUT
# ============================================================================
# Note: Gemini 3 Pro max output is 65,536 tokens
MAX_OUTPUT_TOKENS=65536
MAX_JSON_OUTPUT_TOKENS=65536
ENABLE_JSON_VALIDATION=true
MAX_JSON_REPAIR_ATTEMPTS=3
TOKEN_WARNING_THRESHOLD=50000
INPUT_TOKEN_WARNING=180000

# ============================================================================
# DATABASE & STORAGE
# ============================================================================
DATABASE_URL=sqlite:./data/mira.db
MIRA_SQLITE_MAX_CONNECTIONS=100

# ============================================================================
# QDRANT VECTOR DATABASE
# ============================================================================
# 3 collections: code, conversation, git (each prefixed with QDRANT_COLLECTION)
# Use gRPC port (6334) for qdrant-client, REST port is 6333
QDRANT_URL=http://localhost:6334
QDRANT_COLLECTION=mira
QDRANT_EMBEDDING_DIM=3072

# ============================================================================
# SESSION & USER
# ============================================================================
MIRA_SESSION_ID=default-session
MIRA_DEFAULT_PERSONA=Default

# ============================================================================
# MEMORY & HISTORY
# ============================================================================
MIRA_HISTORY_MESSAGE_CAP=100
MIRA_HISTORY_TOKEN_LIMIT=131072
MIRA_MAX_RETRIEVAL_TOKENS=32768
MIRA_CONTEXT_RECENT_MESSAGES=50
MIRA_CONTEXT_SEMANTIC_MATCHES=25
MIRA_RECENT_MESSAGE_LIMIT=50

# ============================================================================
# MEMORY SERVICE
# ============================================================================
MEM_ALWAYS_EMBED_USER=true
MEM_ALWAYS_EMBED_ASSISTANT=true
MEM_EMBED_MIN_CHARS=6
MEM_DEDUP_SIM_THRESHOLD=0.97
MEM_SALIENCE_MIN_FOR_EMBED=0.5
MEM_ROLLUP_EVERY=100
MIN_SALIENCE_FOR_QDRANT=0.5

# ============================================================================
# EMBEDDING CONFIGURATION
# ============================================================================
MIRA_EMBED_MODEL=gemini-embedding-001
MIRA_EMBED_DIMENSIONS=3072
MIRA_EMBED_HEADS=code,conversation,git
# Set to true to embed code from chat responses (may increase embedding costs)
MIRA_EMBED_CODE_FROM_CHAT=false

# ============================================================================
# MEMORY DECAY
# ============================================================================
MIRA_DECAY_RECENT_HALF_LIFE_DAYS=30.0
MIRA_DECAY_GENTLE_FACTOR=0.1
MIRA_DECAY_STRONGER_FACTOR=0.3
MIRA_DECAY_FLOOR=0.2
MIRA_DECAY_HIGH_SALIENCE_THRESHOLD=0.7

# ============================================================================
# RECALL & CONTEXT
# ============================================================================
MIRA_RECALL_RECENT=10
MIRA_RECALL_SEMANTIC=20
MIRA_RECALL_K_PER_HEAD=10

# ============================================================================
# SUMMARIZATION
# ============================================================================
MIRA_ENABLE_SUMMARIZATION=true
MIRA_SUMMARY_TOKEN_LIMIT=64000
MIRA_SUMMARY_OUTPUT_TOKENS=8192
MIRA_SUMMARIZE_AFTER_MESSAGES=20
MIRA_SUMMARY_ROLLING_10=true
MIRA_SUMMARY_ROLLING_100=true
MIRA_USE_ROLLING_SUMMARIES_IN_CONTEXT=true
MIRA_ROLLING_SUMMARY_MAX_AGE_HOURS=168
MIRA_ROLLING_SUMMARY_MIN_GAP=3

# ============================================================================
# VECTOR SEARCH
# ============================================================================
MIRA_MAX_VECTOR_RESULTS=10
MIRA_ENABLE_VECTOR_SEARCH=true

# ============================================================================
# TOOLS
# ============================================================================
MIRA_ENABLE_CHAT_TOOLS=true
MIRA_ENABLE_WEB_SEARCH=true
MIRA_ENABLE_CODE_INTERPRETER=true
MIRA_ENABLE_FILE_SEARCH=true
MIRA_ENABLE_IMAGE_GENERATION=true
MIRA_WEB_SEARCH_MAX_RESULTS=20
MIRA_TOOL_TIMEOUT_SECONDS=60
TOOL_MAX_ITERATIONS=25

# ============================================================================
# SERVER
# ============================================================================
MIRA_HOST=0.0.0.0
MIRA_PORT=3001
MIRA_MAX_CONCURRENT_EMBEDDINGS=25

# ============================================================================
# RATE LIMITING
# ============================================================================
# Enable rate limiting for API requests
RATE_LIMIT_ENABLED=true
# Standard rate limit (requests per minute)
# Note: Gemini 3 Pro Tier 1 limit is 50 RPM
RATE_LIMIT_REQUESTS_PER_MINUTE=50
# Enable tiered rate limiting (different limits for large context)
RATE_LIMIT_TIERED_ENABLED=true
# Rate limit for large context requests (>200k tokens)
# More conservative to avoid API throttling
RATE_LIMIT_LARGE_CONTEXT_RPM=25

# ============================================================================
# TIMEOUTS
# ============================================================================
GEMINI_TIMEOUT=600
QDRANT_TIMEOUT=60
DATABASE_TIMEOUT=30

# ============================================================================
# LOGGING
# ============================================================================
MIRA_LOG_LEVEL=debug

# ============================================================================
# RESPONSE CONFIGURATION
# ============================================================================
# Note: Gemini 3 Pro max output is 65,536 tokens
MIRA_MAX_RESPONSE_TOKENS=65536

# ============================================================================
# PERFORMANCE & CACHING
# ============================================================================
MIRA_API_MAX_RETRIES=3
MIRA_API_RETRY_DELAY_MS=1000
MIRA_ENABLE_REQUEST_CACHE=true
MIRA_CACHE_TTL_SECONDS=300

# ============================================================================
# RECENT CACHE
# ============================================================================
MIRA_ENABLE_RECENT_CACHE=true
MIRA_RECENT_CACHE_CAPACITY=100
MIRA_RECENT_CACHE_TTL=300
MIRA_RECENT_CACHE_MAX_PER_SESSION=50
MIRA_RECENT_CACHE_WARMUP=true

# ============================================================================
# BACKGROUND TASKS
# ============================================================================
TASK_ANALYSIS_ENABLED=true
TASK_ANALYSIS_INTERVAL=10
TASK_DECAY_ENABLED=true
TASK_DECAY_INTERVAL=14400
TASK_CLEANUP_ENABLED=true
TASK_CLEANUP_INTERVAL=3600
TASK_SESSION_MAX_AGE_HOURS=168
TASK_SUMMARY_ENABLED=true
TASK_SUMMARY_INTERVAL=300
TASK_CODE_SYNC_ENABLED=true
TASK_CODE_SYNC_INTERVAL=300
ACTIVE_SESSION_LIMIT=100

# ============================================================================
# EXTRA ENV VARS
# ============================================================================
RUST_LOG=info
