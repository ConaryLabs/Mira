# .env.example
# Mira Environment Configuration
# Copy to ~/.mira/.env and fill in your values
#
# Configuration is loaded once at startup from:
#   1. ~/.mira/.env (global)
#   2. .env (project-level, overrides global)

# ═══════════════════════════════════════════════════════════════════════════════
# API Keys
# ═══════════════════════════════════════════════════════════════════════════════

# DeepSeek - Primary LLM provider for experts and reasoning
# Get from: https://platform.deepseek.com/api_keys
DEEPSEEK_API_KEY=sk-your-deepseek-key-here

# OpenAI - Required for semantic search (embeddings via text-embedding-3-small)
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-key-here

# Gemini - Usable as an LLM provider for experts
# Get from: https://aistudio.google.com/app/apikey
# GEMINI_API_KEY=your-gemini-key-here

# Brave Search - Enables web search for expert consultations
# Get from: https://brave.com/search/api/
# BRAVE_API_KEY=your-brave-search-key-here

# ═══════════════════════════════════════════════════════════════════════════════
# LLM Provider Configuration
# ═══════════════════════════════════════════════════════════════════════════════

# Override default LLM provider (options: deepseek, gemini)
# By default, Mira uses DeepSeek if available, then falls back to Gemini
# DEFAULT_LLM_PROVIDER=deepseek

# For more fine-grained control, use ~/.mira/config.toml:
#   [llm]
#   expert_provider = "deepseek"      # For expert consultations
#   background_provider = "gemini"    # For summaries, briefings

# ═══════════════════════════════════════════════════════════════════════════════
# User Identity
# ═══════════════════════════════════════════════════════════════════════════════

# Override user identity for multi-user memory scoping
# By default, Mira uses git config user.email or system username
# MIRA_USER_ID=your-unique-id

# ═══════════════════════════════════════════════════════════════════════════════
# Advanced Options
# ═══════════════════════════════════════════════════════════════════════════════

# Disable all LLM features (use heuristic fallbacks only)
# MIRA_DISABLE_LLM=1

# Override embedding dimensions (default: 1536)
# MIRA_EMBEDDING_DIMENSIONS=1536

# Override embedding task type (ignored for OpenAI text-embedding-3-small)
# MIRA_EMBEDDING_TASK_TYPE=RETRIEVAL_DOCUMENT

# Enable fuzzy fallback when embeddings are unavailable (default: true)
# MIRA_FUZZY_FALLBACK=true

# Override project path detection
# MIRA_PROJECT_PATH=/path/to/project

# Alternative to GEMINI_API_KEY for LLM (either works)
# GOOGLE_API_KEY=your-google-key-here

# ═══════════════════════════════════════════════════════════════════════════════
# Expert Guardrails
# ═══════════════════════════════════════════════════════════════════════════════
# These control runtime limits for expert agentic loops.
# Values can only go lower than defaults, never higher (clamped to ceilings).

# Maximum agentic loop iterations per expert (default: 100, ceiling: 500)
# MIRA_EXPERT_MAX_TURNS=100

# Overall expert consultation timeout in seconds (default: 600, ceiling: 1800)
# MIRA_EXPERT_TIMEOUT_SECS=600

# Individual LLM call timeout in seconds (default: 360, ceiling: 600)
# MIRA_LLM_CALL_TIMEOUT_SECS=360

# Maximum concurrent expert consultations (default: 3, ceiling: 10)
# MIRA_MAX_CONCURRENT_EXPERTS=3

# Maximum characters per tool result before truncation (default: 16000, ceiling: 64000)
# MIRA_TOOL_RESULT_MAX_CHARS=16000

# MCP tool call timeout in seconds (default: 60, ceiling: 300)
# MIRA_MCP_TOOL_TIMEOUT_SECS=60

# Maximum parallel tool calls per iteration (default: 8, ceiling: 20)
# MIRA_MAX_PARALLEL_TOOL_CALLS=8

# Maximum total tool calls across all iterations (default: 200, ceiling: 1000)
# MIRA_MAX_TOTAL_TOOL_CALLS=200
